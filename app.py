import streamlit as st
from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer
from wordcloud import WordCloud
import matplotlib.pyplot as plt
import pandas as pd
import plotly.express as px
import re
from deep_translator import GoogleTranslator
from nltk.corpus import stopwords
import nltk
import logging
import chardet
from sklearn.feature_extraction.text import CountVectorizer
from sklearn.decomposition import LatentDirichletAllocation
from reportlab.lib.pagesizes import letter
from reportlab.lib.styles import getSampleStyleSheet
from reportlab.platypus import Paragraph
from reportlab.lib.units import inch
from reportlab.pdfgen import canvas
import io

# Download NLTK stopwords
try:
    nltk.data.find('corpora/stopwords')
except LookupError:
    nltk.download('stopwords')

# Configure logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# --- Page Configuration ---
st.set_page_config(
    page_title="DuoScan P√©dagogique", # MODIFICATION: enlev√© "Avanc√©" ici aussi pour coh√©rence
    page_icon="ü¶â",
    layout="wide"
)

# --- Utility Functions ---
def nettoyer_texte(texte):
    if not texte or not isinstance(texte, str):
        return ""
    texte = texte.lower()
    texte = re.sub(r'[^\w\s]', '', texte)
    texte = re.sub(r'\d+', '', texte)
    stop_words = set(stopwords.words('french')) | set([
        'duolingo', 'lapplication', 'cest', 'app', 'jai', 'suis', 'cest', '√©tait',
        'plus', 'tr√®s', 'fait', 'faire', 'fais', 'faites',
        'bien', 'bon', 'bonne', 'super', 'g√©nial', 'excellent',
        'mauvais', 'nul', 'horrible',
        'toujours', 'merci', 'vraiment', 'svp', 'sil vous pla√Æt',
        '√ßa', 'cela', 'ceci', 'ici',
        '√™tre', 'avoir', 'vouloir', 'pouvoir', 'devoir',
        'depuis', 'peut', 'aussi', 'comme', 'idem',
        'pourquoi', 'quand', 'comment', 'donc', 'alors',
        'tout', 'tous', 'toute', 'toutes', 'rien', 'personne',
        'fois', 'jour', 'jours', 'semaine', 'mois', 'ann√©e',
        'compte', 'probl√®me', 'question', 'chose', 'truc', 'machin',
        'utilisateur', 'interface', 'utilisation', 'version', 'option', 'fonctionnalit√©',
        'd√©but', 'fin', 'milieu', 'partie', 'niveau', 'le√ßon', 'exercice',
        'dit', 'dire', 'parler', 'voir', 'mettre', 'prendre', 'donner'
    ])
    texte = " ".join(word for word in texte.split() if word not in stop_words and len(word) > 1)
    return texte

def translate_to_english(texte):
    if not texte or not isinstance(texte, str) or not texte.strip():
        return ""
    try:
        if len(texte) > 4800:
            logger.warning(f"Texte trop long ({len(texte)} char.) pour traduction auto, analyse VADER sur original si lexique FR pertinent.")
            return texte
        return GoogleTranslator(source='fr', target='en').translate(texte)
    except Exception as e:
        logger.error(f"Translation error for text snippet '{texte[:50]}...': {e}")
        return texte

analyzer = SentimentIntensityAnalyzer()

def analyser_sentiment(texte_original):
    if not texte_original or not isinstance(texte_original, str) or not texte_original.strip():
        return "Neutre üòê"
    texte_original_lower = texte_original.lower()
    negative_terms_fr = {
        'abus√©', 'aga√ßant', 'aga√ßante', 'arnaque', 'arr√™te pas', 'arr√™ter de',
        'attendre longtemps', 'aucun progr√®s', 'aucun sens', 'aucune logique',
        'affreux', 'affreuse', 'bug', 'bugg√©', 'beug', 'beugue',
        'casse les pieds', 'c√©tait mieux avant', 'chiant', 'complexe', 'compliqu√©',
        'd√©bile', 'd√©√ßu', 'd√©cevante', 'd√©croire', 'd√©courageant', 'd√©go√ªt√©',
        'demande toujours', 'd√©pens√© pour rien', 'd√©sagr√©able', 'd√©testable', 'difficile',
        'dommage', 'ennuyeux', 'ennuyeuse', 'erreur', 'erreurs fr√©quentes',
        'fatigant', 'faux', 'forc√© de', 'frustrant', 'frustrante',
        'g√¢che', 'g√™nant', 'horrible', 'honteux',
        'impossible', 'inaccessible', 'inacceptable', 'incompr√©hensible', 'incoh√©rent',
        'incorrect', 'infest√© de pub', 'injouable', 'insupportable', 'instable', 'insuffisant',
        'inutile', 'lent', 'lenteur', 'limit√©',
        'mal fait', 'mal fichu', 'malheureusement', 'manque de', 'marche pas', 'mauvais', 'mauvaise',
        'moche', 'mensonge', 'm√©diocre',
        'ne fonctionne plus', 'ne marche plus', 'ne r√©pond pas', 'n√©gatif',
        'nul', 'nulle', 'nuls', 'oblig√© de', 'obsol√®te',
        'pas assez', 'pas clair', 'pas du tout', 'pas efficace', 'pas terrible', 'pas top',
        'p√©nible', 'perdu', 'perte de temps', 'pire', 'plante', 'plein de bugs', 'popups',
        'pourri', 'pourrie', 'probl√®me', 'probl√®mes',
        'ralentit', 'rame', 'regrettable', 'r√©gress√©', 'ridicule',
        'sans int√©r√™t', 'saturation', 'satur√© de pub', 'souci', 'stupide',
        'trop de pub', 'trop cher', 'trop difficile', 'trop simple', 'triste',
        'usant', 'usine √† gaz', 'vide', 'vieillot', 'violent', 'zero'
    }
    if any(term in texte_original_lower for term in negative_terms_fr):
        if not (("n'est pas" in texte_original_lower or "ne pas" in texte_original_lower or "pas si" in texte_original_lower) and \
                any(term in texte_original_lower for term in ['nul', 'mauvais', 'd√©cevant', 'horrible'])):
            return "N√©gatif üëé"

    texte_en = translate_to_english(texte_original)
    if texte_en == texte_original or not texte_en:
        pass

    vs = analyzer.polarity_scores(texte_en)
    compound_score = vs['compound']

    if compound_score >= 0.05:
        return "Positif üëç"
    elif compound_score <= -0.04:
        return "N√©gatif üëé"
    else:
        subtle_negative_hints_fr = {'un peu d√©√ßu', 'pas vraiment top', 'pourrait √™tre mieux', 'bof'}
        if any(hint in texte_original_lower for hint in subtle_negative_hints_fr):
            return "N√©gatif üëé"
        return "Neutre üòê"

def generer_nuage_mots(textes, titre_section):
    if not textes:
        st.info(f"Pas de donn√©es suffisantes pour g√©n√©rer le nuage de mots {titre_section.lower()}.")
        return
    texte_concatene = " ".join(nettoyer_texte(txt) for txt in textes if txt and isinstance(txt, str))
    if not texte_concatene.strip():
        st.info(f"Pas de mots significatifs (apr√®s nettoyage) pour le nuage {titre_section.lower()}.")
        return
    try:
        wordcloud = WordCloud(
            width=800, height=400, background_color='rgba(255, 255, 255, 0)',
            collocations=True, prefer_horizontal=0.9, min_font_size=10,
        ).generate(texte_concatene)
        fig, ax = plt.subplots(figsize=(10, 5))
        ax.imshow(wordcloud, interpolation='bilinear')
        ax.axis("off")
        st.pyplot(fig)
    except ValueError as ve:
         st.warning(f"Impossible de g√©n√©rer le nuage de mots '{titre_section}' : {ve}. ")
    except Exception as e:
        st.error(f"Erreur lors de la g√©n√©ration du nuage de mots '{titre_section}': {e}")

def detect_encoding(file):
    try:
        raw_data = file.read(100000)
        result = chardet.detect(raw_data)
        file.seek(0)
        if result['encoding'] and result['confidence'] > 0.7:
            return result['encoding']
        logger.info(f"Chardet low confidence ({result['confidence']}) for detected encoding '{result['encoding']}'. Defaulting to utf-8.")
        return 'utf-8'
    except Exception as e:
        logger.error(f"Erreur lors de la d√©tection de l‚Äôencodage : {e}")
        return 'utf-8'

def detect_topics(textes, num_topics=5, n_top_words=7):
    valid_texts = [txt for txt in textes if txt and isinstance(txt, str)]
    if not valid_texts or len(valid_texts) < 2: return []
    try:
        textes_nettoyes = [nettoyer_texte(txt) for txt in valid_texts]
        textes_nettoyes = [txt for txt in textes_nettoyes if txt and len(txt.split()) > 3]
        if len(textes_nettoyes) < num_topics:
            num_topics = max(1, len(textes_nettoyes))
        if len(textes_nettoyes) < 2:
            logger.info("Pas assez de documents valides pour la d√©tection de th√®mes apr√®s nettoyage.")
            return []
        vectorizer = CountVectorizer(max_df=0.90, min_df=3, stop_words='english', ngram_range=(1,2), max_features=1000)
        doc_term_matrix = vectorizer.fit_transform(textes_nettoyes)
        if doc_term_matrix.shape[0] < num_topics or doc_term_matrix.shape[1] == 0:
            logger.warning(f"Matrice document-terme ({doc_term_matrix.shape}) insuffisante pour LDA avec {num_topics} th√®mes.")
            return []
        if doc_term_matrix.shape[1] < n_top_words :
             n_top_words = doc_term_matrix.shape[1]
        lda = LatentDirichletAllocation(n_components=num_topics, random_state=42, learning_method='online', batch_size=128)
        lda.fit(doc_term_matrix)
        feature_names = vectorizer.get_feature_names_out()
        topics_summary = []
        for topic_idx, topic_weights in enumerate(lda.components_):
            top_words_indices = topic_weights.argsort()[:-min(n_top_words + 1, len(feature_names) + 1):-1]
            top_words = [feature_names[i] for i in top_words_indices]
            topics_summary.append(f"Th√®me {topic_idx + 1}: {', '.join(top_words)}")
        return topics_summary
    except Exception as e:
        logger.error(f"Erreur lors de la d√©tection des th√®mes : {e}")
        return []

def analyze_lxd_aspects(textes, sentiments):
    lxd_aspects = {
        "Interface Utilisateur (UI)": ["navigation", "intuitive", "clair", "visuel", "menu", "bouton", "ergonomie", "design pattern"],
        "Personnalisation & Adaptabilit√©": ["personnalis√©", "adapt√©", "objectif", "rythme", "niveau", "adaptive", "sur mesure", "parcours"],
        "Esth√©tique & Engagement √âmotionnel": ["design", "couleur", "ludique", "attrayant", "hibou", "mascotte", "amusant", "plaisant", "joli", "graphisme"],
        "Gamification & Motivation": ["points", "xp", "gemmes", "lingots", "badges", "s√©ries", "streak", "classement", "ligue", "d√©fi", "r√©compense", "vies", "coeurs", "challenge"],
        "Contenu P√©dagogique & Exercices": ["contenu", "cours", "vocabulaire", "grammaire", "phrases", "traduction", "r√©p√©titif", "vari√©t√© des exercices", "qualit√© du contenu"],
        "Feedback & Progression": ["feedback", "correction", "explication", "aide", "progression", "progr√®s", "suivi", "encouragement"],
        "Accessibilit√© & Flexibilit√©": ["mobile", "gratuit", "freemium", "rapide", "courtes", "temps", "hors ligne", "accessibilit√©"],
        "Publicit√© & Mod√®le √âconomique": ["pub", "publicit√©", "premium", "abonnement", "payant", "gratuit vs payant", "co√ªt"]
    }
    aspect_sentiments = {aspect: {"Positif": 0, "N√©gatif": 0, "Neutre": 0, "Mentions":0} for aspect in lxd_aspects}
    for texte, sentiment in zip(textes, sentiments):
        if not texte or not isinstance(texte, str): continue
        texte_lower_cleaned = nettoyer_texte(texte).lower()
        texte_original_lower = texte.lower()
        sentiment_key = sentiment.split(" ")[0]
        for aspect, keywords in lxd_aspects.items():
            if any(keyword in texte_lower_cleaned or keyword in texte_original_lower for keyword in keywords):
                if sentiment_key in aspect_sentiments[aspect]:
                    aspect_sentiments[aspect][sentiment_key] += 1
                aspect_sentiments[aspect]["Mentions"] +=1
    return aspect_sentiments

def generate_theory_recommendations(avis_negatifs_textes, avis_positifs_textes, lxd_results):
    recommendations = []
    texte_neg_concatene = " ".join(nettoyer_texte(txt).lower() for txt in avis_negatifs_textes if txt)
    negative_themes_to_theories = {
        "vies": ("Th√©orie du Flow & Auto-D√©termination", "Le syst√®me de vies/c≈ìurs est un point de friction majeur. Il interrompt le flow (Csikszentmihalyi) et peut nuire au sentiment de comp√©tence (SDT). Proposer un mode 'entra√Ænement libre' sans vies ou des options pour regagner des vies plus facilement est crucial."),
        "bloqu√©": ("Th√©orie du Flow", "Les blocages dus aux erreurs ou au syst√®me de vies interrompent le flow. Envisager des aides contextuelles plus pouss√©es, des 'jokers', ou des mini-r√©visions cibl√©es pour surmonter les obstacles et maintenir l'engagement."),
        "publicit√©s": ("Th√©orie de l‚ÄôAuto-D√©termination & Flow", "Les publicit√©s fr√©quentes sont per√ßues comme une nuisance majeure, nuisant √† l'autonomie et interrompant le flow. R√©duire leur fr√©quence/intrusivit√© ou offrir des contreparties claires (ex: gagner des vies) pour les visionnages volontaires."),
        "r√©p√©titif": ("Critique du Behaviorisme & Th√©orie Cognitive de l'Apprentissage Multim√©dia (Mayer)", "La r√©p√©tition excessive engendre monotonie et d√©sengagement. Introduire plus de vari√©t√© dans les types d'exercices (cf. LXD aspect 'Contenu P√©dagogique') et contextualiser davantage l'apprentissage (sc√©narios, dialogues) pour favoriser un encodage profond et r√©duire la charge cognitive extrins√®que."),
        "m√©canique": ("Critique du Behaviorisme", "Un apprentissage per√ßu comme trop m√©canique limite le transfert des comp√©tences. Int√©grer des t√¢ches qui sollicitent la r√©flexion critique ou la cr√©ativit√© (composition libre, r√©solution de probl√®mes en contexte)."),
        "manque contexte": ("Critique du Behaviorisme & Constructivisme", "Le manque de contexte nuit √† la compr√©hension et √† l'application r√©elle. Renforcer l'utilisation de sc√©narios authentiques, d'histoires interactives plus √©labor√©es et de mises en situation pratiques."),
        "pas explication": ("Behaviorisme (Feedback) & Cognitivisme", "Un feedback insuffisant ou peu clair est un frein majeur. Fournir des explications grammaticales et lexicales d√©taill√©es, accessibles √† la demande, et multimodales (texte, audio, exemples concrets). Le feedback doit √™tre constructif et guider l'apprenant."),
        "grammaire difficile": ("Cognitivisme & Feedback", "Les difficult√©s grammaticales n√©cessitent un soutien p√©dagogique accru. Proposer des mini-le√ßons de grammaire cibl√©es, des tableaux r√©capitulatifs, des exemples clairs et des exercices sp√©cifiques pour chaque point complexe. Le s√©quen√ßage doit √™tre progressif."),
        "pression": ("Th√©orie de l‚ÄôAuto-D√©termination (Comp√©tence & Autonomie)", "La pression des streaks ou des classements peut √™tre anxiog√®ne. Offrir des options pour d√©sactiver/masquer ces √©l√©ments comp√©titifs ou proposer des modes d'apprentissage ax√©s sur la ma√Ætrise personnelle plut√¥t que la comp√©tition."),
        "classement": ("Th√©orie de l‚ÄôAuto-D√©termination (Comp√©tence & Relation)", "Les classements peuvent d√©motiver ceux qui ne sont pas en t√™te. Envisager des formes de 'comp√©tition saine' (ex: d√©fis de groupe contre un objectif commun) ou des comparaisons de progr√®s personnels pour renforcer la relatedness positivement."),
        "notifications": ("Th√©orie de l‚ÄôAuto-D√©termination (Autonomie)", "Les notifications per√ßues comme trop insistantes ou non pertinentes peuvent nuire au sentiment d'autonomie. Permettre une personnalisation fine des rappels et s'assurer qu'ils sont per√ßus comme un soutien et non un contr√¥le."),
        "interaction": ("Constructivisme Social (Vygotsky)", "Un manque d'interaction sociale est souvent relev√©. Explorer des fonctionnalit√©s d'apprentissage collaboratif mod√©r√© : forums de discussion par le√ßon, correction par les pairs (simple), d√©fis collaboratifs."),
        "parler": ("Constructivisme & Apprentissage Actif", "La difficult√© √† transf√©rer les acquis √† l'oral est une pr√©occupation majeure. Int√©grer significativement plus d'exercices de production orale (reconnaissance vocale am√©lior√©e, sc√©narios de dialogue interactif, feedback sur la prononciation)."),
        "√©crire": ("Constructivisme & Apprentissage Actif", "Le manque de pratique en production √©crite est une limite. Proposer des exercices de r√©daction guid√©e, de r√©sum√©, ou de petites descriptions en lien avec les th√®mes √©tudi√©s."),
        "trop cher": ("Accessibilit√© & Mod√®le Freemium (LXD)", "Le co√ªt de la version premium est un frein important. R√©√©valuer la proposition de valeur du mode gratuit versus payant. Envisager des options d'abonnement plus flexibles ou des fonctionnalit√©s premium accessibles ponctuellement via des 'cr√©dits' gagn√©s."),
        "lent": ("Performance & UX (LXD)", "Les lenteurs et les bugs d√©gradent fortement l'exp√©rience utilisateur. Prioriser l'optimisation des performances de l'application et la correction des bugs signal√©s."),
        "bug": ("Fiabilit√© & UX (LXD)", "Les bugs fr√©quents sapent la confiance et la motivation. Mettre en place un processus rigoureux de tests et de r√©solution rapide des bugs, et communiquer sur les correctifs apport√©s."),
    }
    for aspect, counts in lxd_results.items():
        if counts["Mentions"] > 0:
            negative_ratio_for_aspect = counts["N√©gatif"] / counts["Mentions"]
            if negative_ratio_for_aspect > 0.4 and counts["N√©gatif"] > 3 :
                action = f"L'aspect LXD '{aspect}' re√ßoit une proportion significative d'avis n√©gatifs ({counts['N√©gatif']}/{counts['Mentions']}). Il est crucial d'investiguer les causes sp√©cifiques (voir avis d√©taill√©s) et d'envisager des am√©liorations cibl√©es."
                if aspect == "Publicit√© & Mod√®le √âconomique" and "publicit√©s" not in texte_neg_concatene:
                     recommendations.append(f"üî¨ **LXD Critique & Auto-D√©termination**: {action}")
                elif aspect == "Contenu P√©dagogique & Exercices" and "r√©p√©titif" not in texte_neg_concatene:
                     recommendations.append(f"üî¨ **LXD Critique & Cognitivisme**: {action}")
                elif aspect not in ["Publicit√© & Mod√®le √âconomique", "Contenu P√©dagogique & Exercices"]:
                    recommendations.append(f"üî¨ **LXD Critique**: {action}")

    triggered_recommendations_texts = set(rec.split(": ", 1)[1] for rec in recommendations)
    if texte_neg_concatene:
        for keyword, (theory, recommendation_text) in negative_themes_to_theories.items():
            if re.search(r'\b' + re.escape(keyword) + r'\b', texte_neg_concatene):
                if recommendation_text not in triggered_recommendations_texts:
                    recommendations.append(f"üî¨ **{theory}**: {recommendation_text}")
                    triggered_recommendations_texts.add(recommendation_text)

    if not recommendations and avis_negatifs_textes:
        recommendations.append("üîç **Point d'Attention G√©n√©ral**: Des avis n√©gatifs ont √©t√© d√©tect√©s. Il est conseill√© de les examiner manuellement pour identifier des probl√®mes sp√©cifiques non couverts par les th√®mes automatis√©s. L'am√©lioration continue du feedback, la vari√©t√© des exercices et la gestion de la frustration sont des pistes universelles.")
    elif not avis_negatifs_textes and avis_positifs_textes:
        recommendations.append("üéâ **Excellente R√©ception G√©n√©rale**: Les retours sont majoritairement positifs. Capitalisez sur les aspects pl√©biscit√©s (souvent li√©s √† la gamification et √† la facilit√© d'utilisation) et continuez d'innover en douceur.")
    elif not avis_negatifs_textes and not avis_positifs_textes:
        recommendations.append("‚ÑπÔ∏è **Aucun avis positif ou n√©gatif distinct** n'a √©t√© fourni ou d√©tect√© pour g√©n√©rer des recommandations sp√©cifiques. L'analyse se base sur les avis neutres ou l'ensemble des avis si disponibles.")
    return sorted(list(set(recommendations)))

# ### MODIFICATION SIGNIFICATIVE ### Am√©lioration de la fonction de g√©n√©ration PDF
def generate_pdf(metrics, topics, lxd_results_pdf, theory_recs_pdf):
    buffer = io.BytesIO()
    c = canvas.Canvas(buffer, pagesize=letter)
    width, page_height = letter
    styles = getSampleStyleSheet()

    # Define margins
    left_margin = 0.75 * inch
    right_margin = 0.75 * inch
    top_margin = 0.75 * inch
    bottom_margin = 0.75 * inch
    content_width = width - left_margin - right_margin

    # Define styles
    title_style = styles['h1']
    title_style.alignment = 1 # Center
    title_style.fontSize = 18
    title_style.leading = 22

    heading_style = styles['h2']
    heading_style.fontSize = 14
    heading_style.leading = 18
    heading_style.spaceBefore = 12
    heading_style.spaceAfter = 6

    body_style = styles['Normal']
    body_style.fontSize = 10
    body_style.leading = 14 # Increased leading for better readability
    body_style.spaceAfter = 6

    list_item_style = styles['Normal']
    list_item_style.fontSize = 9
    list_item_style.leading = 12 # Increased leading
    list_item_style.leftIndent = 0.25 * inch # Indent list items

    story = []
    current_height = page_height - top_margin

    # Helper function to add paragraphs and manage height
    def add_paragraph(text, style, story_list, current_h, page_h, top_m, bottom_m):
        p = Paragraph(text.encode('latin-1', 'replace').decode('latin-1'), style)
        p_w, p_h = p.wrapOn(c, content_width, page_h) # Calculate height
        
        if current_h - p_h < bottom_m: # Check if it fits
            story_list.append(PageBreak())
            current_h = page_h - top_m
        story_list.append(p)
        current_h -= p_h
        return current_h

    # Title
    current_height = add_paragraph("Rapport d'Analyse - DuoScan P√©dagogique", title_style, story, current_height, page_height, top_margin, bottom_margin)
    current_height -= 0.2 * inch # Extra space after title

    # Metrics Section
    current_height = add_paragraph("Indicateurs Cl√©s des Sentiments", heading_style, story, current_height, page_height, top_margin, bottom_margin)
    metrics_text = [
        f"Total Avis Analys√©s: {metrics['Total Avis Analys√©s']}",
        f"Avis Positifs: {metrics['Avis Positifs üëç']} ({metrics.get('Pos %', 'N/A')})",
        f"Avis N√©gatifs: {metrics['Avis N√©gatifs üëé']} ({metrics.get('Neg %', 'N/A')})",
        f"Avis Neutres: {metrics['Avis Neutres üòê']} ({metrics.get('Neu %', 'N/A')})"
    ]
    for mt_text in metrics_text:
        current_height = add_paragraph(mt_text, body_style, story, current_height, page_height, top_margin, bottom_margin)
    current_height -= 0.1 * inch

    # Topics Section
    current_height = add_paragraph("Th√®mes Principaux Identifi√©s (LDA)", heading_style, story, current_height, page_height, top_margin, bottom_margin)
    if topics:
        for topic_text in topics:
            current_height = add_paragraph(f"‚Ä¢ {topic_text}", list_item_style, story, current_height, page_height, top_margin, bottom_margin)
    else:
        current_height = add_paragraph("Aucun th√®me principal d√©tect√©.", list_item_style, story, current_height, page_height, top_margin, bottom_margin)
    current_height -= 0.1 * inch

    # LXD Aspects Section
    current_height = add_paragraph("Analyse des Aspects LXD", heading_style, story, current_height, page_height, top_margin, bottom_margin)
    if lxd_results_pdf:
        for aspect, counts in lxd_results_pdf.items():
            aspect_summary = f"‚Ä¢ {aspect}: Pos={counts['Positif']}, N√©g={counts['N√©gatif']}, Neu={counts['Neutre']} (Total mentions: {counts['Mentions']})"
            current_height = add_paragraph(aspect_summary, list_item_style, story, current_height, page_height, top_margin, bottom_margin)
    else:
        current_height = add_paragraph("Aucune donn√©e d'aspect LXD √† afficher.", list_item_style, story, current_height, page_height, top_margin, bottom_margin)
    current_height -= 0.1 * inch

    # Recommendations Section
    current_height = add_paragraph("Recommandations P√©dagogiques & LXD", heading_style, story, current_height, page_height, top_margin, bottom_margin)
    if theory_recs_pdf:
        for rec_idx, rec_text in enumerate(theory_recs_pdf):
            rec_cleaned = rec_text.replace("üî¨ ", "").replace("**", "")
            current_height = add_paragraph(f"{rec_idx + 1}. {rec_cleaned}", list_item_style, story, current_height, page_height, top_margin, bottom_margin)
    else:
        current_height = add_paragraph("Aucune recommandation sp√©cifique g√©n√©r√©e.", list_item_style, story, current_height, page_height, top_margin, bottom_margin)

    # Build PDF using SimpleDocTemplate for better flow control
    from reportlab.platypus import SimpleDocTemplate, PageBreak
    doc = SimpleDocTemplate(buffer, pagesize=letter,
                            leftMargin=left_margin, rightMargin=right_margin,
                            topMargin=top_margin, bottomMargin=bottom_margin)
    try:
        doc.build(story)
    except Exception as e:
        logger.error(f"Erreur majeure lors de la construction du PDF avec SimpleDocTemplate : {e}")
        # Fallback to basic error PDF if build fails
        buffer.seek(0)
        buffer.truncate() # Clear buffer before writing error
        c_error = canvas.Canvas(buffer, pagesize=letter)
        c_error.setFont("Helvetica", 12)
        c_error.drawCentredString(width/2, page_height/2 + 20, "Erreur critique lors de la g√©n√©ration du PDF.")
        error_message_pdf = f"D√©tail: {str(e)[:100]}"
        c_error.drawCentredString(width/2, page_height/2, error_message_pdf.encode('latin-1', 'replace').decode('latin-1'))
        c_error.save()

    buffer.seek(0)
    return buffer


# --- Sidebar ---
with st.sidebar:
    # ### MODIFICATION ### Photo r√©int√©gr√©e
    st.image("https://i.imgur.com/LF3KIQa.jpeg", width=100, caption="EL FILALI MOHAMED")
    # ### MODIFICATION ### "Version Avanc√©e" enlev√© du titre de la sidebar
    st.markdown("# DuoScan P√©dagogique ü¶â")
    st.divider()
    st.markdown("#### Master : Ing√©nierie Technop√©dagogique et Innovation")
    st.markdown("#### Module : Design de l'Exp√©rience d'Apprentissage (LXD)")
    st.markdown("#### Professeur : M. Adil AMMAR")
    st.markdown("#### R√©alis√© par : EL FILALI Mohamed")
    st.divider()
    st.markdown("### Objectif de l'outil:")
    st.success(
        "Analyser finement le sentiment et les th√©matiques des retours utilisateurs "
        "de Duolingo (ou similaire) pour identifier avec pr√©cision des pistes d'am√©lioration "
        "bas√©es sur le LXD et les th√©ories de l'apprentissage."
    )
    st.markdown("### Guide Rapide")
    st.markdown("""
    1.  Choisissez la m√©thode d'entr√©e (texte ou fichier).
    2.  Fournissez les avis (max 1000 pour performance).
    3.  Cliquez sur "**üöÄ Analyser les Avis**".
    4.  Explorez les insights et recommandations g√©n√©r√©s !
    """)
    st.divider()
    # ### MODIFICATION ### Message de date de mise √† jour enlev√©
    # st.info(f"Derni√®re mise √† jour du script : {pd.Timestamp('today').strftime('%d/%m/%Y')}")
    st.caption(f"¬© {pd.Timestamp.now().year} - EL FILALI Mohamed - ITPI")


# --- Main Interface ---
st.title("ü¶â DuoScan P√©dagogique") # MODIFICATION: Titre simplifi√©
st.subheader("Analyse des Avis Utilisateurs pour l'Optimisation LXD") # MODIFICATION: Sous-titre simplifi√©
st.markdown("""
Cet outil permet une analyse s√©mantique et p√©dagogique des avis utilisateurs.
Il se base sur :
- Une d√©tection de sentiments sensible aux nuances.
- L'identification de th√®mes cl√©s (via LDA) et d'aspects LXD pr√©dominants.
- La g√©n√©ration de recommandations actionnables ancr√©es dans les th√©ories de l'apprentissage.
""") # MODIFICATION: Description simplifi√©e
st.divider()

# --- Data Input Section ---
st.header("üì• 1. Soumission des Avis Utilisateurs")
input_method = st.radio(
    "Comment souhaitez-vous fournir les avis ?",
    ("Coller le texte directement", "T√©l√©charger un fichier (.txt, .csv, .xls, .xlsx)"),
    key="input_method_choice",
    horizontal=True
)

avis_entres_bruts = []
MAX_REVIEWS_HARD_LIMIT = 2000
MAX_REVIEWS_DEFAULT_ANALYSIS = 1000

if input_method == "Coller le texte directement":
    avis_texte_area = st.text_area(
        f"Collez ici les avis (un par ligne, {MAX_REVIEWS_DEFAULT_ANALYSIS} recommand√©s, {MAX_REVIEWS_HARD_LIMIT} max.) :",
        height=200, key="text_area_input",
        placeholder=("Exemple 1 : Duolingo m'a vraiment aid√© √† apprendre l'espagnol, c'est ludique et efficace !\n"
                     "Exemple 2 : Je suis d√©√ßu, l'application plante souvent et les publicit√©s sont trop pr√©sentes, c'est frustrant.\n"
                     "Exemple 3 : C'est pas mal mais le syst√®me de vies est un peu d√©courageant parfois.")
    )
    if avis_texte_area:
        avis_entres_bruts = [avis.strip() for avis in avis_texte_area.split('\n') if avis.strip()]

elif input_method.startswith("T√©l√©charger un fichier"):
    uploaded_file = st.file_uploader(
        f"S√©lectionnez un fichier .txt, .csv, .xls ou .xlsx (colonne 'avis', 'review', 'commentaire' ou unique colonne). Limite : {MAX_REVIEWS_HARD_LIMIT} avis.",
        type=["txt", "csv", "xls", "xlsx"], key="file_uploader_input"
    )
    if uploaded_file is not None:
        try:
            filename = uploaded_file.name.lower()
            df_avis = None # Initialize df_avis
            if filename.endswith('.txt'):
                encoding = detect_encoding(uploaded_file)
                avis_entres_bruts = [line.decode(encoding).strip() for line in uploaded_file if line.decode(encoding).strip()]
            elif filename.endswith('.csv'):
                encoding = detect_encoding(uploaded_file)
                try: df_avis = pd.read_csv(uploaded_file, encoding=encoding, sep=None, engine='python')
                except Exception as e_csv:
                    st.error(f"Erreur de lecture CSV : {e_csv}. V√©rifiez l'encodage ({encoding} d√©tect√©) et le s√©parateur.")
                    st.stop()
            elif filename.endswith(('.xls', '.xlsx')):
                try: df_avis = pd.read_excel(uploaded_file, engine='openpyxl' if filename.endswith('.xlsx') else 'xlrd')
                except Exception as e_excel:
                    st.error(f"Erreur de lecture Excel : {e_excel}. Assurez-vous que le fichier n'est pas corrompu.")
                    st.stop()
            
            if df_avis is not None: # Process if df_avis was loaded
                potential_cols = ['avis', 'review', 'reviews', 'commentaire', 'commentaires', 'text', 'content']
                review_col = None
                for col_name in df_avis.columns: # Iterate through actual column names
                    if str(col_name).lower() in potential_cols:
                        review_col = col_name
                        break
                
                if review_col:
                    avis_entres_bruts = [str(avis).strip() for avis in df_avis[review_col].dropna().tolist() if str(avis).strip()]
                elif len(df_avis.columns) == 1:
                    avis_entres_bruts = [str(avis).strip() for avis in df_avis.iloc[:, 0].dropna().tolist() if str(avis).strip()]
                else:
                    st.error(f"Fichier {filename.split('.')[-1].upper()} : impossible de trouver une colonne d'avis pertinente. Veuillez nommer la colonne d'avis de mani√®re explicite ou utiliser un fichier √† une seule colonne.")
                    st.stop()
        except Exception as e:
            st.error(f"‚ö†Ô∏è Erreur majeure lors de la lecture du fichier '{uploaded_file.name}' : {e}. V√©rifiez le format et l'encodage (UTF-8 recommand√©).")
            st.stop()

st.divider()
num_reviews_to_analyze = MAX_REVIEWS_DEFAULT_ANALYSIS
if avis_entres_bruts:
    max_slider_val = min(len(avis_entres_bruts), MAX_REVIEWS_HARD_LIMIT)
    if max_slider_val > 1:
        num_reviews_to_analyze = st.slider(
            f"Nombre d'avis √† analyser (sur {len(avis_entres_bruts)} d√©tect√©s) :", 1, max_slider_val,
            min(MAX_REVIEWS_DEFAULT_ANALYSIS, max_slider_val),
            step=10 if max_slider_val > 100 else 1,
            help=f"Ajustez pour √©quilibrer la profondeur de l'analyse et le temps de traitement. Limite stricte √† {MAX_REVIEWS_HARD_LIMIT}."
        )
    else: num_reviews_to_analyze = max_slider_val

if st.button(f"üöÄ Analyser {num_reviews_to_analyze if avis_entres_bruts else ''} Avis", type="primary", use_container_width=True, key="analyze_button"):
    if not avis_entres_bruts:
        st.warning("‚ö†Ô∏è Veuillez fournir des avis avant de lancer l'analyse.")
        st.stop()
    avis_a_analyser = avis_entres_bruts[:num_reviews_to_analyze]
    if len(avis_entres_bruts) > num_reviews_to_analyze:
        st.info(f"Analyse limit√©e aux {num_reviews_to_analyze} premiers avis sur {len(avis_entres_bruts)} fournis.")

    st.header("üìä 2. R√©sultats de l'Analyse")
    progress_bar = st.progress(0, text="Initialisation de l'analyse...")
    with st.spinner("ü¶â Analyse s√©mantique et th√©matique en cours..."):
        avis_analyses = []
        sentiments_counts = {"Positif üëç": 0, "N√©gatif üëé": 0, "Neutre üòê": 0}
        avis_positifs_textes, avis_negatifs_textes, avis_neutres_textes = [], [], []
        total_avis_pour_analyse = len(avis_a_analyser)
        for i, avis_texte in enumerate(avis_a_analyser):
            progress_text = f"Traitement de l'avis {i+1}/{total_avis_pour_analyse}..."
            progress_bar.progress((i + 1) / total_avis_pour_analyse, text=progress_text)
            sentiment = analyser_sentiment(avis_texte)
            avis_analyses.append({"id": i + 1, "texte_original": avis_texte, "sentiment_detecte": sentiment})
            sentiments_counts[sentiment] += 1
            if sentiment == "Positif üëç": avis_positifs_textes.append(avis_texte)
            elif sentiment == "N√©gatif üëé": avis_negatifs_textes.append(avis_texte)
            else: avis_neutres_textes.append(avis_texte)
        progress_bar.progress(1.0, text="Analyse des sentiments termin√©e ! üéâ")
        st.toast('Analyse des sentiments termin√©e ! üéâ', icon='‚úÖ')

        st.subheader("üìà Vue d'Ensemble des Sentiments")
        total_analyzed = len(avis_analyses)
        metrics_for_pdf = {
            "Total Avis Analys√©s": total_analyzed,
            "Avis Positifs üëç": sentiments_counts["Positif üëç"],
            "Avis N√©gatifs üëé": sentiments_counts["N√©gatif üëé"],
            "Avis Neutres üòê": sentiments_counts["Neutre üòê"],
            "Pos %": f"{((sentiments_counts['Positif üëç']/total_analyzed)*100):.1f}%" if total_analyzed > 0 else "0%",
            "Neg %": f"{((sentiments_counts['N√©gatif üëé']/total_analyzed)*100):.1f}%" if total_analyzed > 0 else "0%",
            "Neu %": f"{((sentiments_counts['Neutre üòê']/total_analyzed)*100):.1f}%" if total_analyzed > 0 else "0%",
        }
        cols_metriques = st.columns(4)
        cols_metriques[0].metric("Avis Analys√©s", metrics_for_pdf["Total Avis Analys√©s"])
        cols_metriques[1].metric("Avis Positifs üëç", metrics_for_pdf["Avis Positifs üëç"], delta=metrics_for_pdf["Pos %"])
        cols_metriques[2].metric("Avis N√©gatifs üëé", metrics_for_pdf["Avis N√©gatifs üëé"], delta=metrics_for_pdf["Neg %"])
        cols_metriques[3].metric("Avis Neutres üòê", metrics_for_pdf["Avis Neutres üòê"], delta=metrics_for_pdf["Neu %"])

        if total_analyzed > 0:
            df_sentiments = pd.DataFrame(list(sentiments_counts.items()), columns=['Sentiment', 'Nombre'])
            sentiment_color_map = {"Positif üëç": "#28a745", "N√©gatif üëé": "#dc3545", "Neutre üòê": "#6c757d"}
            fig = px.pie(df_sentiments, names='Sentiment', values='Nombre', title="R√©partition des Sentiments",
                         color='Sentiment', color_discrete_map=sentiment_color_map, hole=0.3)
            fig.update_traces(textposition='inside', textinfo='percent+label+value')
            fig.update_layout(showlegend=True, paper_bgcolor='rgba(0,0,0,0)', plot_bgcolor='rgba(0,0,0,0)', legend_title_text='Cat√©gories')
            st.plotly_chart(fig, use_container_width=True)
        else: st.info("Aucune donn√©e de sentiment √† afficher.")
        st.divider()
        
        all_avis_textes_analyzed = [avis['texte_original'] for avis in avis_analyses]
        progress_bar.progress(0.33, text="D√©tection des th√®mes principaux (LDA)...")
        with st.spinner("Identification des th√®mes majeurs..."):
            detected_topics_list = detect_topics(all_avis_textes_analyzed, num_topics=5, n_top_words=7)
        progress_bar.progress(0.66, text="Analyse des aspects LXD...")
        with st.spinner("Analyse des aspects LXD..."):
            sentiments_list_for_lxd = [entry["sentiment_detecte"] for entry in avis_analyses]
            lxd_results = analyze_lxd_aspects(all_avis_textes_analyzed, sentiments_list_for_lxd)
        progress_bar.progress(1.0, text="Analyses th√©matiques et LXD termin√©es !")

        with st.expander("üîç Exploration D√©taill√©e des Avis Filtr√©s", expanded=False):
            col_filter1, col_filter2 = st.columns(2)
            sentiment_filter = col_filter1.selectbox("Filtrer par Sentiment", ["Tous", "Positif üëç", "N√©gatif üëé", "Neutre üòê"], key="sentiment_filter_exp")
            topic_options_display = ["Tous"] + [f"{t.split(':')[0]} ({t.split(': ')[1][:30]}...)" for t in detected_topics_list]
            topic_filter_display = col_filter2.selectbox("Filtrer par Mots-Cl√©s d'un Th√®me", topic_options_display, key="topic_filter_exp")
            filtered_avis_df = pd.DataFrame(avis_analyses)
            if sentiment_filter != "Tous":
                filtered_avis_df = filtered_avis_df[filtered_avis_df["sentiment_detecte"] == sentiment_filter]
            if topic_filter_display != "Tous":
                actual_topic_prefix = topic_filter_display.split(" (")[0]
                selected_topic_full = next((t for t in detected_topics_list if t.startswith(actual_topic_prefix)), None)
                if selected_topic_full:
                    topic_keywords = [kw.strip() for kw in selected_topic_full.split(": ")[1].split(",")]
                    filtered_avis_df = filtered_avis_df[
                        filtered_avis_df["texte_original"].str.lower().apply(lambda x: any(kw in x for kw in topic_keywords))
                    ]
            if not filtered_avis_df.empty:
                st.dataframe(
                    filtered_avis_df[['id', 'sentiment_detecte', 'texte_original']].rename(
                        columns={'id':'ID', 'sentiment_detecte':'Sentiment', 'texte_original':'Avis Original'}),
                    use_container_width=True, hide_index=True, height=350
                )
            else: st.info("Aucun avis ne correspond √† vos crit√®res de filtrage.")
        st.divider()

        st.subheader("‚òÅÔ∏è Nuages de Mots : Termes Fr√©quents par Sentiment")
        if not avis_positifs_textes and not avis_negatifs_textes and not avis_neutres_textes:
            st.info("Pas assez de donn√©es textuelles pour g√©n√©rer les nuages de mots.")
        else:
            tab_positif, tab_negatif, tab_neutre = st.tabs(["üü¢ Termes Positifs", "üî¥ Termes N√©gatifs", "‚ö™ Termes Neutres"])
            with tab_positif:
                if avis_positifs_textes: generer_nuage_mots(avis_positifs_textes, "issus des Avis Positifs")
                else: st.info("Aucun avis positif pour g√©n√©rer un nuage de mots.")
            with tab_negatif:
                if avis_negatifs_textes: generer_nuage_mots(avis_negatifs_textes, "issus des Avis N√©gatifs")
                else: st.info("Aucun avis n√©gatif pour g√©n√©rer un nuage de mots.")
            with tab_neutre:
                if avis_neutres_textes: generer_nuage_mots(avis_neutres_textes, "issus des Avis Neutres")
                else: st.info("Aucun avis neutre pour g√©n√©rer un nuage de mots.")
        st.divider()
        
        st.subheader("üéØ Th√®mes Principaux Identifi√©s dans les Avis (via LDA)")
        if detected_topics_list:
            st.markdown("Les th√®mes suivants ont √©t√© extraits automatiquement des avis analys√©s :")
            for topic_item in detected_topics_list: st.markdown(f"- {topic_item}")
        else: st.warning("Analyse th√©matique non concluante : pas assez de donn√©es.")
        st.divider()

        st.subheader("üñåÔ∏è Analyse D√©taill√©e des Aspects LXD")
        if lxd_results:
            st.markdown("R√©partition des sentiments pour les aspects LXD identifi√©s (nombre de mentions) :")
            lxd_df_list = []
            for aspect, counts in lxd_results.items():
                if counts["Mentions"] > 0:
                     lxd_df_list.append({'Aspect LXD': aspect, 'Positif': counts['Positif'], 'N√©gatif': counts['N√©gatif'], 'Neutre': counts['Neutre'], 'Total Mentions': counts['Mentions']})
            if lxd_df_list:
                df_lxd_display = pd.DataFrame(lxd_df_list).sort_values(by="Total Mentions", ascending=False)
                st.dataframe(df_lxd_display, hide_index=True, use_container_width=True)
                if not df_lxd_display.empty:
                    df_lxd_melted = df_lxd_display.melt(id_vars=['Aspect LXD', 'Total Mentions'], value_vars=['Positif', 'N√©gatif', 'Neutre'], var_name='Sentiment', value_name='Nombre d\'Avis')
                    df_lxd_melted = df_lxd_melted[df_lxd_melted['Nombre d\'Avis'] > 0]
                    if not df_lxd_melted.empty:
                        fig_lxd = px.bar(df_lxd_melted, x='Aspect LXD', y='Nombre d\'Avis', color='Sentiment', barmode='group', title="Ventilation des Sentiments par Aspect LXD", color_discrete_map=sentiment_color_map, labels={'Nombre d\'Avis': "Nombre d'Avis Mentionnant l'Aspect"})
                        fig_lxd.update_layout(xaxis_tickangle=-45, yaxis_title="Nombre d'Avis", xaxis_title="Aspect LXD")
                        st.plotly_chart(fig_lxd, use_container_width=True)
                    else: st.info("Aucune mention avec sentiment sp√©cifique pour les aspects LXD √† visualiser.")
            else: st.info("Aucun aspect LXD sp√©cifique n'a pu √™tre clairement identifi√©.")
        else: st.warning("L'analyse LXD n'a pas pu √™tre effectu√©e.")
        st.divider()

        st.header("üí° 3. Synth√®se Interpr√©tative et Recommandations Strat√©giques")
        st.markdown("#### Analyse Quantitative et Qualitative Automatis√©e")
        positive_percentage = (sentiments_counts["Positif üëç"] / total_analyzed * 100) if total_analyzed else 0
        negative_percentage = (sentiments_counts["N√©gatif üëé"] / total_analyzed * 100) if total_analyzed else 0
        if positive_percentage > 65: st.success(f"‚úÖ **Perception Globale Tr√®s Positive ({positive_percentage:.1f}%)**")
        elif positive_percentage > 40: st.info(f"üëç **Perception Globale Positive ({positive_percentage:.1f}%)**")
        else: st.warning(f"‚ö†Ô∏è **Perception Mitig√©e ou N√©gative ({positive_percentage:.1f}% de positifs)**")
        if negative_percentage > 30: st.error(f"üö® **Frustrations Significatives ({negative_percentage:.1f}%)**")
        elif negative_percentage > 15: st.warning(f"üîç **Points de Friction Pr√©sents ({negative_percentage:.1f}%)**")
        else: st.success(f"üòå **Faible Taux de N√©gativit√© ({negative_percentage:.1f}%)**")

        st.markdown("#### Recommandations P√©dagogiques et LXD (Bas√©es sur les Th√©ories de l'Apprentissage)")
        if avis_negatifs_textes or avis_positifs_textes or lxd_results:
            with st.spinner("√âlaboration des recommandations strat√©giques..."):
                theory_recommendations = generate_theory_recommendations(avis_negatifs_textes, avis_positifs_textes, lxd_results)
            if theory_recommendations:
                st.info("Les recommandations suivantes sont g√©n√©r√©es automatiquement...")
                for i, rec in enumerate(theory_recommendations): st.markdown(f"**{i+1}.** {rec}")
            else: st.info("Aucune recommandation sp√©cifique n'a pu √™tre g√©n√©r√©e automatiquement.")
        else: st.info("Pas assez d'avis pour g√©n√©rer des recommandations p√©dagogiques robustes.")
        st.divider()

        st.markdown("### üì• Exportation des Donn√©es et du Rapport")
        col1_exp, col2_exp = st.columns(2)
        with col1_exp:
            if avis_analyses:
                df_export = pd.DataFrame(avis_analyses)[['id', 'sentiment_detecte', 'texte_original']].rename(columns={'id':'ID', 'sentiment_detecte':'Sentiment D√©tect√©', 'texte_original':'Texte de l\'Avis'})
                csv_data = df_export.to_csv(index=False).encode('utf-8')
                st.download_button(label="üì• T√©l√©charger les Avis Analys√©s (CSV)", data=csv_data, file_name=f"resultats_analyse_avis_duolingo_{pd.Timestamp.now().strftime('%Y%m%d')}.csv", mime="text/csv")
        with col2_exp:
            if avis_analyses:
                lxd_results_for_pdf = {k: v for k, v in lxd_results.items() if v["Mentions"] > 0} if 'lxd_results' in locals() and lxd_results else {}
                theory_recs_for_pdf = theory_recommendations if 'theory_recommendations' in locals() and theory_recommendations else []
                pdf_buffer = generate_pdf(metrics_for_pdf, detected_topics_list, lxd_results_for_pdf, theory_recs_for_pdf)
                st.download_button(label="üìÑ T√©l√©charger le Rapport de Synth√®se (PDF)", data=pdf_buffer, file_name=f"rapport_synthese_duoscan_{pd.Timestamp.now().strftime('%Y%m%d')}.pdf", mime="application/pdf")
        st.divider()

        st.markdown("### ‚úçÔ∏è Votre Espace d'Analyse Strat√©gique")
        st.markdown("En vous appuyant sur les donn√©es g√©n√©r√©es, veuillez √©laborer votre propre analyse...")
        st.text_area(
            "R√©digez ici votre analyse approfondie et vos recommandations personnalis√©es pour M. AMMAR :",
            height=300, key="interpretation_area_detailed_final",
            placeholder=("Exemple : L'analyse r√©v√®le une forte corr√©lation entre les avis n√©gatifs sur 'le syst√®me de vies'...")
        )
else:
    st.info("‚ÑπÔ∏è Pr√™t pour une analyse approfondie des avis ? Soumettez vos donn√©es et cliquez sur 'Analyser'.")
    st.markdown("---")
    st.markdown("### √Ä propos de DuoScan P√©dagogique") # MODIFICATION: "Avanc√©" enlev√©
    st.markdown("""
    Cette version de **DuoScan P√©dagogique** vise √† fournir une analyse fine et actionnable des retours utilisateurs.
    Elle met l'accent sur :
    - Une d√©tection de sentiment robuste.
    - Une identification claire des aspects LXD et de leur perception.
    - Des recommandations ancr√©es dans les th√©ories de l'apprentissage et les donn√©es concr√®tes des avis.
    L'objectif est de transformer les avis bruts en intelligence strat√©gique pour l'am√©lioration continue des exp√©riences d'apprentissage.
    """) # MODIFICATION: Description simplifi√©e